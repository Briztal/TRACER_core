The Kalman filter is a recursive estimator, that will at first predict a sequences_initialised, get a measure,
    and compute the new sequences_initialised, depending on incertitude on sequences_initialised and measure.

    We will maintain across the algorithm the following set of variables :
        - X : the sequences_initialised (vector);
        - P : the covariance matrix of the error on sequences_initialised (matrix);
        - M : the measure (vector);

    We will also use three constant matrices :
        - H : the translation matrix, used to compute the measure from the sequences_initialised;
        - Q : The covariance matrix of the process_t noise;
        - R : The covariance matrix of the measure noise;


Step 0 : the prediction.

    The current sequences_initialised X of the system is known (initial sequences_initialised given, or previously computed).

    A prediction is made, from this current sequences_initialised :

        X+ = A * X (vector)

    The covariance matrix is updated, as the random variable of the sequences_initialised is known to follow the following rule :
        VX+ = A * VX + Pn (Random variable)
            with VX is the random variable of the sequences_initialised, Pn is a centered gaussian random variable,
            representing the process_t noise;

        P+ = A * P * At + Q (matrix)
            with At the transposed of A, Q the covariance matrix of the process_t noise;



Step 1 : Measure.

    A new measure M is made.

    The prediction is translated to a prediction on the measure, with the affine_curve relation :
        M+ = H * X+ (vector);
            with H the translation matrix of sequences_initialised to measure, M+ the predicted measure, X+ the predicted sequences_initialised;

    The innovation is given by :
        Y = M - M+ = M - H * X (vector)
            with M the new measure;



Step 2 : Computation.

    We now determine the covariance matrix of the innovation, as the random variable of the measure is known
        to follow the following rule :

        VM = H * VX + Mn (Random variable)

        with VX is the random variable of the sequences_initialised, VM the random variable of the measure, Mn is a centered gaussian
            random variable, representing the measure noise;

        S = H * P+ * Ht + R (matrix)

        with S the covariance matrix of the innovation, P+ the updated covariance matrix, R the covariance matrix of
            the measure noise;


    Then, we determine the optimal Kalman gain :

        K = P+ * Ht * S-1 (matrix)

        with P+ the updated covariance matrix, Ht the transposed of H, S-1 the inverse of S;



Step 3 : Update.

    We can now update the current sequences_initialised, with the following formula :

        newX = X+ + K * Y (vector)

        with newX the new sequences_initialised, X+ the predicted sequences_initialised, K the Kalman gain, Y the innovation;


    Finally, we can update the covariance matrix with the following formula :

        newP = (I - K * H) * P+ (matrix)

        with I the identity matrix, K the Kalman gain, H the transformation matrix, P+ the covariance of the predicted
            sequences_initialised;

